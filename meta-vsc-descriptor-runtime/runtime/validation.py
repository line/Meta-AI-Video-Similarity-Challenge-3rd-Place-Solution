#!/usr/bin/env python3
"""
Descriptor Track validation script
"""
import logging
from argparse import ArgumentParser, Namespace

import numpy as np
import pandas as pd

parser = ArgumentParser()
parser.add_argument(
    "--query_features",
    help="Path containing query features",
    type=str,
    required=True,
)
parser.add_argument(
    "--ref_features",
    help="Path containing reference features",
    type=str,
    required=True,
)
parser.add_argument(
    "--query_metadata",
    help="Path containing query metadata",
    type=str,
    required=True,
)
parser.add_argument(
    "--ref_metadata",
    help="Path containing reference metadata",
    type=str,
    required=True,
)
parser.add_argument("--subset", help="Path containing query subset ids", type=str)


logging.basicConfig(
    format="%(asctime)s %(levelname)-8s %(message)s",
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("descriptor_eval_lib.py")
logger.setLevel(logging.INFO)


class DataValidationError(AssertionError):
    pass


def validate_total_descriptors(dataset: str, n_features: int, total_seconds: float):
    if n_features > total_seconds:
        raise DataValidationError(
            f"Number of {dataset} video features must not exceed one feature per second. "
            f"Saw {n_features} vectors, max allowed is {total_seconds}"
        )


def validate_lengths(dataset: str, features_npz):
    n_video_ids = len(features_npz["video_ids"])
    n_timestamps = len(features_npz["timestamps"])
    n_features = len(features_npz["features"])
    if not (n_video_ids == n_timestamps == n_features):
        raise DataValidationError(
            f"Arrays lengths for {dataset} do not match. "
            f"video_ids: {n_video_ids}; "
            f"timestamps: {n_timestamps}; "
            f"features: {n_features}. "
        )


def validate_descriptor_dtype(dataset: str, features_array: np.ndarray):
    if features_array.dtype != np.float32:
        raise DataValidationError(
            f"Features array for {dataset} is not float32. "
            f"dtype is: {features_array.dtype}"
        )


def validate_descriptor_dim(
    dataset: str, features_array: np.ndarray, max_dim: int = 512
):
    if (submitted_dim := features_array.shape[1]) > max_dim:
        raise DataValidationError(
            f"Features array for {dataset} exceeds max dim of {max_dim}: "
            f"submitted dim is {submitted_dim}."
        )


def validate_sorted_ids(dataset: str, video_ids: np.array):
    is_sorted = video_ids[:-1] <= video_ids[1:]
    if not np.all(is_sorted):
        indices = np.argwhere(~is_sorted)
        raise DataValidationError(f"Video ids not sorted at index {indices[0]}.")


def main(args: Namespace):
    query_features = np.load(args.query_features, allow_pickle=False)
    ref_features = np.load(args.ref_features, allow_pickle=False)
    query_meta = pd.read_csv(args.query_metadata)
    ref_meta = pd.read_csv(args.ref_metadata)
    if args.subset:
        subset = pd.read_csv(args.subset)
        query_meta = query_meta.set_index("video_id").loc[subset.video_id]
    query_total_seconds = query_meta.duration_sec.apply(np.ceil).sum()
    ref_total_seconds = ref_meta.duration_sec.apply(np.ceil).sum()

    validate_total_descriptors(
        "query", query_features["features"].shape[0], query_total_seconds
    )
    validate_total_descriptors(
        "reference", ref_features["features"].shape[0], ref_total_seconds
    )

    validate_lengths("query", query_features)
    validate_lengths("reference", ref_features)

    validate_sorted_ids("query", query_features["video_ids"])
    validate_sorted_ids("reference", ref_features["video_ids"])

    validate_descriptor_dtype("query", query_features["features"])
    validate_descriptor_dtype("reference", ref_features["features"])

    validate_descriptor_dim("query", query_features["features"], max_dim=512)
    validate_descriptor_dim("reference", ref_features["features"], max_dim=512)


if __name__ == "__main__":
    args = parser.parse_args()
    main(args)
